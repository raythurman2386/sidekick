{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sidekick - Your Local AI Desktop Companion","text":"<p>Sidekick is an intelligent conversational assistant for your desktop, powered by local AI through Ollama. It provides a seamless desktop experience for interacting with various AI models while keeping all your data private and secure on your local machine.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udda5\ufe0f Standalone desktop application with an intuitive user interface built with Custom Tkinter</li> <li>\ud83d\ude80 Fast responses with local AI processing</li> <li>\ud83d\udd12 Complete privacy - all processing happens on your machine</li> <li>\ud83d\udcbe Conversation history stored in local SQLite database</li> <li>\ud83e\udeb6 Lightweight and efficient performance</li> <li>\ud83c\udf10 Cross-platform support for Windows, Mac and Linux</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Prerequisites</li> <li>Installation Guide</li> <li>Configuration</li> <li>Running Sidekick</li> <li>Troubleshooting</li> </ul>"},{"location":"#support","title":"Support","text":"<p>If you need help or have questions, please:</p> <ul> <li>Check our documentation</li> <li>Open an issue</li> <li>Review the troubleshooting guide</li> </ul>"},{"location":"contributing/contributing/","title":"Contributing","text":"<p>We welcome contributions to Sidekick! This guide will help you get started with contributing to the project.</p>"},{"location":"contributing/contributing/#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Fork the repository</li> <li>Create a new branch for your feature</li> <li>Make your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone your fork:    <pre><code>git clone https://github.com/YOUR_USERNAME/sidekick.git\ncd sidekick\n</code></pre></p> </li> <li> <p>Create a virtual environment:    <pre><code>python -m venv venv\n# Windows\n.\\venv\\Scripts\\activate\n# Mac/Linux\nsource venv/bin/activate\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"contributing/contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use meaningful variable and function names</li> <li>Add comments for complex logic</li> <li>Write clear commit messages</li> </ul>"},{"location":"contributing/contributing/#testing","title":"Testing","text":"<ul> <li>Add tests for new features</li> <li>Ensure existing tests pass</li> <li>Test on multiple platforms if possible</li> </ul>"},{"location":"contributing/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Update documentation if needed</li> <li>Add tests for new features</li> <li>Ensure CI passes</li> <li>Request review from maintainers</li> </ol>"},{"location":"contributing/contributing/#questions","title":"Questions?","text":"<p>Feel free to: - Open an issue - Ask questions in pull requests - Contact the maintainers</p>"},{"location":"installation/configuration/","title":"Configuration","text":""},{"location":"installation/configuration/#environment-setup","title":"Environment Setup","text":"<p>Create a <code>.env</code> file in the project root with the following settings:</p> <pre><code>OLLAMA_HOST=\"http://localhost:11434\"\nAI_MODEL=\"deepseek-r1:latest\"  # or your preferred model\n</code></pre>"},{"location":"installation/configuration/#configuration-options","title":"Configuration Options","text":""},{"location":"installation/configuration/#ollama_host","title":"OLLAMA_HOST","text":"<ul> <li>Default: <code>http://localhost:11434</code></li> <li>Description: The URL where Ollama is running</li> <li>Note: Change this if you're running Ollama on a different port or host</li> </ul>"},{"location":"installation/configuration/#ai_model","title":"AI_MODEL","text":"<ul> <li>Default: <code>deepseek-r1:latest</code></li> <li>Description: The AI model to use for conversations</li> <li>Available options:</li> <li><code>deepseek-r1:latest</code></li> <li><code>phi:latest</code></li> <li>Any other model installed through Ollama</li> </ul>"},{"location":"installation/configuration/#verifying-configuration","title":"Verifying Configuration","text":"<ol> <li>Ensure Ollama is running in the background</li> <li>Verify your model is properly installed:    <pre><code>ollama list\n</code></pre></li> <li>Check that your <code>.env</code> file is in the correct location</li> <li>Make sure the model specified in your <code>.env</code> file matches an installed model</li> </ol>"},{"location":"installation/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Running Sidekick</li> <li>Troubleshooting Configuration Issues</li> </ul>"},{"location":"installation/guide/","title":"Installation Guide","text":"<p>Sidekick provides automated installation scripts that will set up everything you need, including Ollama and a default AI model. Choose the appropriate method for your operating system:</p>"},{"location":"installation/guide/#automated-installation","title":"Automated Installation","text":""},{"location":"installation/guide/#windows","title":"Windows","text":"<ol> <li>Open PowerShell as Administrator</li> <li>Navigate to the project directory</li> <li>Run: <pre><code>Set-ExecutionPolicy RemoteSigned -Scope Process; .\\install.ps1\n</code></pre></li> </ol>"},{"location":"installation/guide/#linuxmacos","title":"Linux/MacOS","text":"<ol> <li>Open Terminal</li> <li>Navigate to the project directory</li> <li>Run: <pre><code>chmod +x install.sh\n./install.sh\n</code></pre></li> </ol> <p>The installation script will: - Install Ollama if not already installed - Download and set up a small Llama2 model - Create a Python virtual environment - Install all required dependencies - Build the executable using PyInstaller</p> <p>After installation completes, you'll find the Sidekick executable in the <code>dist</code> folder.</p>"},{"location":"installation/guide/#manual-installation","title":"Manual Installation","text":"<p>If you prefer to install components manually, follow these steps:</p> <ol> <li> <p>Create a virtual environment: <pre><code>python -m venv venv\n# On Windows\n.\\venv\\Scripts\\activate\n# On Mac/Linux\nsource venv/bin/activate\n</code></pre></p> </li> <li> <p>Install the requirements: <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"installation/guide/#next-steps","title":"Next Steps","text":"<ul> <li>Configure Sidekick</li> <li>Running Sidekick</li> <li>Troubleshooting</li> </ul>"},{"location":"installation/prerequisites/","title":"Prerequisites","text":"<p>Before installing Sidekick, you need to set up Ollama on your system:</p> <ol> <li>Install Ollama from ollama.com</li> <li>You can install AI models in two ways:</li> <li>Through the UI: Open Sidekick's settings (\u2699\ufe0f) and use the \"Install New Model\" section. Enter the model name (e.g., <code>phi:latest</code>) and click \"Install Model\". Please be patient during the download as models can be quite large (several gigabytes).</li> <li>Through the command line:      <pre><code>ollama pull deepseek-r1:latest\n# or\nollama pull phi:latest\n</code></pre></li> </ol>"},{"location":"installation/prerequisites/#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>Sufficient disk space for AI models (several GB depending on the model)</li> <li>Operating System: Windows, Mac, or Linux</li> </ul>"},{"location":"license/license/","title":"License","text":"<p>This project is licensed under the BSD 3-Clause License.</p> <p>See the LICENSE file for the full license text.</p>"},{"location":"roadmap/roadmap/","title":"Roadmap","text":"<p>This document outlines the planned features and improvements for Sidekick. The roadmap is subject to change based on user feedback and project priorities.</p>"},{"location":"roadmap/roadmap/#in-progress","title":"In Progress \ud83d\udea7","text":""},{"location":"roadmap/roadmap/#bug-fixes-and-improvements","title":"Bug Fixes and Improvements","text":"<ul> <li>Ongoing bug fixes and performance improvements</li> <li>Enhanced error handling and user feedback</li> <li>System stability improvements</li> </ul>"},{"location":"roadmap/roadmap/#uiux-enhancements","title":"UI/UX Enhancements","text":"<ul> <li>Markdown support for message box</li> <li>Rich text formatting</li> <li>Code block syntax highlighting</li> <li>Support for lists and tables</li> <li>Improved message rendering</li> <li>Better conversation flow visualization</li> </ul>"},{"location":"roadmap/roadmap/#short-term-goals","title":"Short-term Goals \ud83c\udfaf","text":""},{"location":"roadmap/roadmap/#model-tools-and-integration","title":"Model Tools and Integration","text":"<ul> <li>Add tool usage capabilities for AI models</li> <li>Implement function calling</li> <li>Enhanced context management</li> <li>Model performance optimization</li> <li>Support for more Ollama models</li> </ul>"},{"location":"roadmap/roadmap/#image-generation","title":"Image Generation","text":"<ul> <li>Integration with local image generation models</li> <li>Basic image editing capabilities</li> <li>Image prompt optimization</li> <li>Save and export generated images</li> <li>Image history management</li> </ul>"},{"location":"roadmap/roadmap/#future-plans","title":"Future Plans \ud83c\udf1f","text":""},{"location":"roadmap/roadmap/#enhanced-features","title":"Enhanced Features","text":"<ul> <li>Voice input/output support</li> <li>Custom model fine-tuning interface</li> <li>Advanced prompt templates</li> <li>Conversation categorization and tagging</li> <li>Export conversations in multiple formats</li> </ul>"},{"location":"roadmap/roadmap/#developer-tools","title":"Developer Tools","text":"<ul> <li>Code completion integration</li> <li>Built-in code execution environment</li> <li>Version control system integration</li> <li>Project management features</li> <li>Code analysis and suggestions</li> </ul>"},{"location":"roadmap/roadmap/#user-experience","title":"User Experience","text":"<ul> <li>Customizable themes and layouts</li> <li>Keyboard shortcuts and gestures</li> <li>Context-aware suggestions</li> <li>Multi-language support</li> <li>Conversation search and filtering</li> </ul>"},{"location":"roadmap/roadmap/#productivity-features","title":"Productivity Features","text":"<ul> <li>Calendar and task integration</li> <li>Document summarization</li> <li>Meeting notes generation</li> <li>Email composition assistance</li> <li>Research and citation tools</li> </ul>"},{"location":"roadmap/roadmap/#advanced-ai-features","title":"Advanced AI Features","text":"<ul> <li>Multi-model conversations</li> <li>Automated workflow creation</li> <li>Data visualization tools</li> <li>Custom model training interface</li> <li>API integration tools</li> </ul>"},{"location":"roadmap/roadmap/#community-suggestions","title":"Community Suggestions \ud83d\udca1","text":"<p>We welcome feature suggestions from our community! If you have ideas for new features or improvements:</p> <ol> <li>Open an issue on GitHub</li> <li>Tag it with \"feature-request\"</li> <li>Provide detailed description and use cases</li> </ol>"},{"location":"roadmap/roadmap/#priority-guidelines","title":"Priority Guidelines","text":"<p>Features are prioritized based on:</p> <ul> <li>User impact and demand</li> <li>Technical feasibility</li> <li>Resource requirements</li> <li>Strategic alignment</li> </ul>"},{"location":"roadmap/roadmap/#contributing","title":"Contributing","text":"<p>Want to help implement these features? Check our Contributing Guide to get started!</p>"},{"location":"troubleshooting/troubleshooting/","title":"Troubleshooting","text":"<p>This guide helps you resolve common issues you might encounter while using Sidekick.</p>"},{"location":"troubleshooting/troubleshooting/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"troubleshooting/troubleshooting/#1-ollama-connection-issues","title":"1. Ollama Connection Issues","text":"<p>Problem: Unable to connect to Ollama</p> <p>Solutions: 1. Ensure Ollama is running in the background 2. Verify the Ollama host URL in your <code>.env</code> file 3. Check if Ollama is running on the correct port (default: 11434) 4. Restart Ollama service</p>"},{"location":"troubleshooting/troubleshooting/#2-model-installation-issues","title":"2. Model Installation Issues","text":"<p>Problem: Model fails to install or load</p> <p>Solutions: 1. Verify your model is properly installed:    <pre><code>ollama list\n</code></pre> 2. Check available disk space 3. Try pulling the model again:    <pre><code>ollama pull your-model-name\n</code></pre> 4. Ensure your system meets the model's requirements</p>"},{"location":"troubleshooting/troubleshooting/#3-application-wont-start","title":"3. Application Won't Start","text":"<p>Problem: Sidekick fails to launch</p> <p>Solutions: 1. Check Python version (3.10+ required) 2. Verify all dependencies are installed:    <pre><code>pip install -r requirements.txt\n</code></pre> 3. Check the <code>.env</code> file configuration 4. Look for error messages in the console</p>"},{"location":"troubleshooting/troubleshooting/#4-performance-issues","title":"4. Performance Issues","text":"<p>Problem: Slow responses or high resource usage</p> <p>Solutions: 1. Try a smaller model 2. Close unnecessary applications 3. Check system resources 4. Verify no other processes are competing for GPU resources</p>"},{"location":"troubleshooting/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you're still experiencing issues:</p> <ol> <li>Check the documentation</li> <li>Open an issue on GitHub</li> <li>Include relevant information:</li> <li>Error messages</li> <li>System specifications</li> <li>Steps to reproduce the issue</li> <li>Logs (if available)</li> </ol>"},{"location":"usage/building/","title":"Building from Source","text":"<p>This guide explains how to build Sidekick from source code to create a standalone executable.</p>"},{"location":"usage/building/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have: - Python 3.10 or higher installed - Git (to clone the repository) - PyInstaller (installed via requirements.txt)</p>"},{"location":"usage/building/#building-steps","title":"Building Steps","text":"<ol> <li> <p>Clone the repository (if you haven't already):    <pre><code>git clone https://github.com/raythurman2386/sidekick.git\ncd sidekick\n</code></pre></p> </li> <li> <p>Create and activate a virtual environment:    <pre><code>python -m venv venv\n# Windows\n.\\venv\\Scripts\\activate\n# Mac/Linux\nsource venv/bin/activate\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Build the executable:    <pre><code>pyinstaller --onefile --windowed --icon=\"src/images/sidekick.ico\" --noconsole --hidden-import=tkinter --name=\"Sidekick\" --add-data=\"src/images:images\" src/main.py\n</code></pre></p> </li> </ol>"},{"location":"usage/building/#build-options-explained","title":"Build Options Explained","text":"<ul> <li><code>--onefile</code>: Creates a single executable file</li> <li><code>--windowed</code>: Prevents a console window from appearing</li> <li><code>--icon</code>: Sets the application icon</li> <li><code>--noconsole</code>: Hides the console window</li> <li><code>--hidden-import</code>: Includes necessary imports</li> <li><code>--add-data</code>: Includes additional resources</li> </ul>"},{"location":"usage/building/#output","title":"Output","text":"<p>The built executable will be located in the <code>dist</code> directory. You can run it directly from there.</p>"},{"location":"usage/building/#verifying-the-build","title":"Verifying the Build","text":"<ol> <li>Navigate to the <code>dist</code> directory</li> <li>Run the Sidekick executable</li> <li>Verify that:</li> <li>The application starts without errors</li> <li>The UI appears correctly</li> <li>All features are working as expected</li> </ol>"},{"location":"usage/building/#troubleshooting-build-issues","title":"Troubleshooting Build Issues","text":"<p>If you encounter issues during the build:</p> <ol> <li> <p>Ensure all dependencies are installed:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Clear PyInstaller cache:    <pre><code># Windows\nrmdir /s /q build\n# Mac/Linux\nrm -rf build/\n</code></pre></p> </li> <li> <p>Try rebuilding with the verbose flag:    <pre><code>pyinstaller --onefile --windowed --icon=\"src/images/sidekick.ico\" --noconsole --hidden-import=tkinter --name=\"Sidekick\" --add-data=\"src/images:images\" src/main.py -v\n</code></pre></p> </li> </ol>"},{"location":"usage/how-it-works/","title":"How It Works","text":"<p>Sidekick is designed to provide a seamless, private AI experience on your desktop. Here's a detailed look at how the system works.</p>"},{"location":"usage/how-it-works/#architecture-overview","title":"Architecture Overview","text":""},{"location":"usage/how-it-works/#core-components","title":"Core Components","text":"<ol> <li>User Interface</li> <li>Built with Custom Tkinter</li> <li>Provides an intuitive chat interface</li> <li> <p>Includes settings management</p> </li> <li> <p>Local AI Processing</p> </li> <li>Powered by Ollama</li> <li>All processing happens on your machine</li> <li> <p>No data leaves your system</p> </li> <li> <p>Data Storage</p> </li> <li>Uses SQLite for conversation history</li> <li>Local storage ensures privacy</li> <li>Efficient data management</li> </ol>"},{"location":"usage/how-it-works/#data-flow","title":"Data Flow","text":"<ol> <li>User Input</li> <li>User enters text through the UI</li> <li> <p>Input is processed locally</p> </li> <li> <p>AI Processing</p> </li> <li>Request sent to local Ollama instance</li> <li>Model processes the input</li> <li> <p>Response generated locally</p> </li> <li> <p>Storage</p> </li> <li>Conversation saved to SQLite</li> <li>History available for context</li> <li>All data remains on your machine</li> </ol>"},{"location":"usage/how-it-works/#privacy-security","title":"Privacy &amp; Security","text":"<ul> <li>No cloud processing</li> <li>No data transmission</li> <li>Complete local control</li> <li>Secure data storage</li> </ul>"},{"location":"usage/how-it-works/#performance","title":"Performance","text":"<p>Sidekick is designed to be: - Lightweight - Resource-efficient - Fast and responsive - Cross-platform compatible</p>"},{"location":"usage/how-it-works/#technical-details","title":"Technical Details","text":"<ul> <li>Built with Python 3.10+</li> <li>Uses Custom Tkinter for UI</li> <li>Integrates with Ollama API</li> <li>SQLite for data persistence</li> </ul>"},{"location":"usage/running/","title":"Running Sidekick","text":""},{"location":"usage/running/#starting-the-application","title":"Starting the Application","text":"<p>After installation, you can start Sidekick in one of two ways:</p>"},{"location":"usage/running/#using-the-executable","title":"Using the Executable","text":"<p>If you installed Sidekick using the automated installation scripts, you'll find the executable in the <code>dist</code> folder. Simply double-click the executable to start the application.</p>"},{"location":"usage/running/#using-python","title":"Using Python","text":"<p>If you installed Sidekick manually, you can start it using Python:</p> <pre><code># Ensure you're in the virtual environment\n# Windows\n.\\venv\\Scripts\\activate\n# Mac/Linux\nsource venv/bin/activate\n\n# Start the application\npython main.py\n</code></pre>"},{"location":"usage/running/#first-run","title":"First Run","text":"<ol> <li>When you first start Sidekick, it will:</li> <li>Check for Ollama installation</li> <li>Verify the configured AI model is available</li> <li> <p>Initialize the local SQLite database for conversation history</p> </li> <li> <p>The main interface will appear, showing:</p> </li> <li>Chat input area</li> <li>Settings button (\u2699\ufe0f)</li> <li>Conversation history (if any)</li> </ol>"},{"location":"usage/running/#using-sidekick","title":"Using Sidekick","text":"<ol> <li>Type your message in the input area</li> <li>Press Enter or click Send to start a conversation</li> <li>The AI will process your input locally and respond</li> <li>Your conversation will be automatically saved</li> </ol>"},{"location":"usage/running/#settings","title":"Settings","text":"<p>Access settings by clicking the \u2699\ufe0f button: - Change AI models - Install new models - Configure system settings</p>"},{"location":"usage/running/#next-steps","title":"Next Steps","text":"<ul> <li>Building from Source</li> <li>How It Works</li> <li>Troubleshooting</li> </ul>"}]}